{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base imports\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Compute imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy import spatial as sp\n",
    "from scipy.spatial.distance import hamming, squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly import express as px\n",
    "\n",
    "# ML import\n",
    "import prince\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, confusion_matrix, auc\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from pyphylon.util import load_config\n",
    "\n",
    "from pyphylon.pangenome import get_gene_frequency_submatrices, connectivity\n",
    "from pyphylon.models import run_nmf, normalize_nmf_outputs, binarize_nmf_outputs, generate_nmf_reconstructions, calculate_nmf_reconstruction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = load_config(\"config.yml\")\n",
    "WORKDIR = CONFIG[\"WORKDIR\"]\n",
    "SPECIES = CONFIG[\"PG_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genes = pd.read_pickle(os.path.join(WORKDIR, f'processed/cd-hit-results/{SPECIES}_strain_by_gene.pickle.gz'))\n",
    "df_genes.fillna(0, inplace=True)\n",
    "df_genes = df_genes.sparse.to_dense().astype('int8')\n",
    "\n",
    "df_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(os.path.join(WORKDIR, 'interim/enriched_metadata_2d.csv'), index_col=0, dtype='object')\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter metadata for Complete sequences only\n",
    "metadata_complete = metadata[metadata.genome_status == 'Complete'] # filter for only Complete sequences\n",
    "\n",
    "# # Filter P matrix for Complete sequences only\n",
    "df_genes_complete = df_genes[metadata_complete.genome_id]\n",
    "inCompleteseqs = df_genes_complete.sum(axis=1) > 0 # filter for genes found in complete sequences\n",
    "df_genes_complete = df_genes_complete[inCompleteseqs]\n",
    "\n",
    "df_genes_complete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sparse representations of the P matrix\n",
    "df_genes_complete_sparse = df_genes_complete.astype(pd.SparseDtype(\"int8\", 0))\n",
    "\n",
    "coo_genes = df_genes_complete_sparse.sparse.to_coo()\n",
    "csr_genes = csr_matrix(coo_genes)\n",
    "csr_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse P matrix into a frequency matrix\n",
    "df_genes_freq = pd.DataFrame(index=df_genes_complete_sparse.index, data=csr_genes.sum(axis=1), columns=['freq'])\n",
    "df_genes_freq = df_genes_freq.freq\n",
    "df_genes_freq.sort_values().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import (full) accessory genome\n",
    "df_acc_complete = pd.read_csv(os.path.join(WORKDIR, 'processed/CAR_genomes/df_acc.csv'), index_col=0)\n",
    "df_acc_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submatrices of accessory genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_submatrices = get_gene_frequency_submatrices(df_acc_complete)\n",
    "P_submatrices[0][100] = None # this is just the full accessory genome, we can remove this \"submatrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCA (for rank analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Perform MCA with prince (run once)\n",
    "mca = prince.MCA(\n",
    "    n_components=df_acc_complete.shape[1],  # Set the number of components\n",
    "    n_iter=3,           # Set the number of iterations for the CA algorithm\n",
    "    copy=True,\n",
    "    check_input=True,\n",
    "    engine='sklearn',\n",
    "    random_state=42\n",
    ")\n",
    "mca = mca.fit(df_acc_complete)  # Fit MCA on the dataframe\n",
    "\n",
    "mca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract and plot the cumulative explained variance\n",
    "explained_variance_percentage = mca.percentage_of_variance_  # Retrieve exp. inertia which gives variance explained by each component\n",
    "cumulative_variance = pd.Series(explained_variance_percentage).cumsum()  # Cumulative sum to find cumulative explained inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot (full)\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cumulative_variance, marker='o', linestyle='-', color='blue')\n",
    "plt.title('Cumulative Explained Variance by Dimension')\n",
    "plt.xlabel('Number of Dimensions')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "\n",
    "# Calculate explained inertia (variance) thresholds\n",
    "threshold = {num: cumulative_variance[cumulative_variance >= num].index[0] for num in range(1,99)}\n",
    "\n",
    "# Add vertical lines for explained inertia thresholds\n",
    "plt.axvline(x=threshold[70], color='grey', label=f'70% Explained Variance: {threshold[70]}', linestyle='--')\n",
    "plt.axvline(x=threshold[75], color='limegreen', label=f'75% Explained Variance: {threshold[75]}', linestyle='--')\n",
    "plt.axvline(x=threshold[80], color='purple', label=f'80% Explained Variance: {threshold[80]}', linestyle='--')\n",
    "plt.axvline(x=threshold[85], color='pink', label=f'85% Explained Variance: {threshold[85]}', linestyle='--')\n",
    "plt.axvline(x=threshold[90], color='maroon', label=f'90% Explained Variance: {threshold[90]}', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot (first n components)\n",
    "n_significant_components = (explained_variance_percentage > 0.01).sum()\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cumulative_variance[:n_significant_components], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Cumulative Explained Inertia (Variance) by Dimension')\n",
    "plt.xlabel('Number of Dimensions')\n",
    "plt.ylabel('Cumulative Explained Inertia (Variance)')\n",
    "\n",
    "# Add vertical lines for explained inertia thresholds\n",
    "plt.axvline(x=threshold[70], color='grey', label=f'70% Explained Variance: {threshold[70]}', linestyle='--')\n",
    "plt.axvline(x=threshold[75], color='limegreen', label=f'75% Explained Variance: {threshold[75]}', linestyle='--')\n",
    "plt.axvline(x=threshold[80], color='purple', label=f'80% Explained Variance: {threshold[80]}', linestyle='--')\n",
    "plt.axvline(x=threshold[85], color='pink', label=f'85% Explained Variance: {threshold[85]}', linestyle='--')\n",
    "plt.axvline(x=threshold[90], color='maroon', label=f'90% Explained Variance: {threshold[90]}', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASH_RANK = 16 # Mash rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_list = sorted(set([\n",
    "    2,\n",
    "    MASH_RANK,\n",
    "    threshold[70],\n",
    "    threshold[75],\n",
    "    threshold[80],\n",
    "    threshold[85],\n",
    "    threshold[90],\n",
    "]))\n",
    "\n",
    "rank_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF decomposition on accessory genome\n",
    "\n",
    "- NMF at various ranks\n",
    "- Find \"best\" model\n",
    "- NMF around \"best\" model\n",
    "- Repeat for NMF of submatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial NMF decomposition across ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_dict, H_dict = run_nmf(\n",
    "    data=df_acc_complete,\n",
    "    ranks=rank_list,\n",
    "    max_iter=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_norm_dict, A_norm_dict = normalize_nmf_outputs(df_acc_complete, W_dict, H_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_binarized_dict, A_binarized_dict = binarize_nmf_outputs(L_norm_dict, A_norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_reconstructed_dict, P_error_dict, P_confusion_dict = generate_nmf_reconstructions(df_acc_complete, L_binarized_dict, A_binarized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = calculate_nmf_reconstruction_metrics(P_reconstructed_dict, P_confusion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.sort_values(by='AIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerunning with extra values near \"best rank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New ranks\n",
    "best_rank = df_metrics['AIC'].idxmin(axis=0)\n",
    "extra_ranks = rank_list.copy() + list(range(best_rank-3, best_rank+3+1))\n",
    "\n",
    "# NMF run\n",
    "W_dict, H_dict = run_nmf(\n",
    "    data=df_acc_complete,\n",
    "    ranks=extra_ranks,\n",
    "    max_iter=10_000\n",
    ")\n",
    "\n",
    "# Postprocess\n",
    "L_norm_dict, A_norm_dict = normalize_nmf_outputs(df_acc_complete, W_dict, H_dict)\n",
    "L_binarized_dict, A_binarized_dict = binarize_nmf_outputs(L_norm_dict, A_norm_dict)\n",
    "\n",
    "# Reconstruction & error metrics\n",
    "P_reconstructed_dict, P_error_dict, P_confusion_dict = generate_nmf_reconstructions(df_acc_complete, L_binarized_dict, A_binarized_dict)\n",
    "df_metrics = calculate_nmf_reconstruction_metrics(P_reconstructed_dict, P_confusion_dict)\n",
    "\n",
    "df_metrics.sort_values(by='AIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rank_by_aic = df_metrics['AIC'].idxmin(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running NMF with extra rank list on submatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize submatrix dict of dicts\n",
    "W_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "H_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "\n",
    "L_norm_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "A_norm_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "\n",
    "L_binarized_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "A_binarized_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "\n",
    "P_reconstructed_submatrices =  dict.fromkeys(P_submatrices.keys())\n",
    "P_error_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "P_confusion_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "\n",
    "df_metrics_submatrices = dict.fromkeys(P_submatrices.keys())\n",
    "\n",
    "\n",
    "# Helper function to make dict of dicts\n",
    "def make_dict_in_dict(d: dict):\n",
    "    keys = sorted(d.keys())\n",
    "    for key in keys:\n",
    "        d[key] = dict.fromkeys(keys)\n",
    "\n",
    "# List for helper function\n",
    "dod_list = [\n",
    "    W_submatrices, H_submatrices,\n",
    "    L_norm_submatrices, A_norm_submatrices,\n",
    "    L_binarized_submatrices, A_binarized_submatrices,\n",
    "    P_reconstructed_submatrices, P_error_submatrices, P_confusion_submatrices,\n",
    "    df_metrics_submatrices\n",
    "]\n",
    "\n",
    "# Make dict of dicts\n",
    "for dod in dod_list:\n",
    "    make_dict_in_dict(dod)\n",
    "\n",
    "# Actual NMF decomposition\n",
    "for min_key in tqdm(P_submatrices.keys(), desc='Iterating over min keys'):\n",
    "    for max_key in tqdm(P_submatrices.keys(), desc='Iterating over max keys'):\n",
    "        if min_key == 0 and max_key == 100:\n",
    "            continue\n",
    "        if min_key < max_key:\n",
    "            # NMF run\n",
    "            W_submatrices[min_key][max_key], H_submatrices[min_key][max_key] = run_nmf(\n",
    "                data=P_submatrices[min_key][max_key],\n",
    "                ranks=extra_ranks,\n",
    "                max_iter=10_000\n",
    "            )\n",
    "            \n",
    "            # Postprocess\n",
    "            L_norm_submatrices[min_key][max_key], A_norm_submatrices[min_key][max_key] = normalize_nmf_outputs(\n",
    "                P_submatrices[min_key][max_key],\n",
    "                W_submatrices[min_key][max_key],\n",
    "                H_submatrices[min_key][max_key]\n",
    "            )\n",
    "            L_binarized_submatrices[min_key][max_key], A_binarized_submatrices[min_key][max_key] = binarize_nmf_outputs(\n",
    "                L_norm_submatrices[min_key][max_key],\n",
    "                A_norm_submatrices[min_key][max_key]\n",
    "            )\n",
    "            \n",
    "            # Reconstruction & error metrics\n",
    "            a, b, c = generate_nmf_reconstructions(\n",
    "                P_submatrices[min_key][max_key],\n",
    "                L_binarized_submatrices[min_key][max_key],\n",
    "                A_binarized_submatrices[min_key][max_key]\n",
    "            )\n",
    "            P_reconstructed_submatrices[min_key][max_key] = a\n",
    "            P_error_submatrices[min_key][max_key] = b\n",
    "            P_confusion_submatrices[min_key][max_key] = c\n",
    "            \n",
    "            df_metrics_submatrices[min_key][max_key] = calculate_nmf_reconstruction_metrics(\n",
    "                P_reconstructed_submatrices[min_key][max_key],\n",
    "                P_confusion_submatrices[min_key][max_key]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of valid ranges\n",
    "l = []\n",
    "\n",
    "for x in sorted(P_submatrices.keys()):\n",
    "    for y in sorted(P_submatrices.keys()):\n",
    "        if x == 0 and y == 100:\n",
    "            continue\n",
    "        elif x < y:\n",
    "            l.append((x, y))\n",
    "\n",
    "# Initialize dicts\n",
    "best_ranks_dict = dict.fromkeys(l)\n",
    "best_ranks_dict['full'] = df_metrics['AIC'].idxmin(axis=0)\n",
    "\n",
    "best_L_norm_dict = dict.fromkeys(l)\n",
    "best_L_norm_dict['full'] = L_norm_dict[df_metrics['AIC'].idxmin(axis=0)]\n",
    "\n",
    "best_A_norm_dict = dict.fromkeys(l)\n",
    "best_A_norm_dict['full'] = A_norm_dict[df_metrics['AIC'].idxmin(axis=0)]\n",
    "\n",
    "best_L_binarized_dict = dict.fromkeys(l)\n",
    "best_L_binarized_dict['full'] = L_binarized_dict[df_metrics['AIC'].idxmin(axis=0)]\n",
    "\n",
    "best_A_binarized_dict = dict.fromkeys(l)\n",
    "best_A_binarized_dict['full'] = A_binarized_dict[df_metrics['AIC'].idxmin(axis=0)]\n",
    "\n",
    "# Get best ranks and models\n",
    "for min_key in P_submatrices.keys():\n",
    "    for max_key in P_submatrices.keys():\n",
    "        if min_key == 0 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key < max_key:\n",
    "            print(f'submatrix range: {min_key, max_key}')\n",
    "            print(f\"best rank by AIC: {df_metrics_submatrices[min_key][max_key]['AIC'].idxmin(axis=0)}\")\n",
    "            \n",
    "            best_ranks_dict[(min_key, max_key)] = df_metrics_submatrices[min_key][max_key]['AIC'].idxmin(axis=0)\n",
    "            best_L_norm_dict[(min_key, max_key)] = L_norm_submatrices[min_key][max_key][best_ranks_dict[(min_key, max_key)]]\n",
    "            best_A_norm_dict[(min_key, max_key)] = A_norm_submatrices[min_key][max_key][best_ranks_dict[(min_key, max_key)]]\n",
    "            \n",
    "            best_L_binarized_dict[(min_key, max_key)] = L_binarized_submatrices[min_key][max_key][best_ranks_dict[(min_key, max_key)]]\n",
    "            best_A_binarized_dict[(min_key, max_key)] = A_binarized_submatrices[min_key][max_key][best_ranks_dict[(min_key, max_key)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics['AIC'].idxmin(axis=0) # Best rank for the full accessory genome (for comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding robust clusters of strains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus matrix With all submatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_dict = dict.fromkeys(P_submatrices.keys())\n",
    "make_dict_in_dict(conn_dict)\n",
    "\n",
    "for min_key in P_submatrices.keys():\n",
    "    for max_key in P_submatrices.keys():\n",
    "        if min_key == 0 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key < max_key:\n",
    "            conn_dict[min_key][max_key] = connectivity(\n",
    "                P_submatrices[min_key][max_key].values,\n",
    "                A_binarized_submatrices[min_key][max_key][best_ranks_dict[(min_key, max_key)]].values\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consensus matrix for these runs (H matrix, default)\n",
    "consensus_matrix = np.zeros(shape=conn_dict[0][25].shape)\n",
    "\n",
    "num_conn_mat = 0\n",
    "for min_key in P_submatrices.keys():\n",
    "    for max_key in P_submatrices.keys():\n",
    "        if min_key == 0 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key < max_key:\n",
    "            num_conn_mat += 1\n",
    "            consensus_matrix += conn_dict[min_key][max_key]\n",
    "\n",
    "consensus_matrix /= num_conn_mat\n",
    "\n",
    "df_consensus_matrix = pd.DataFrame(consensus_matrix, index=df_acc_complete.columns, columns=df_acc_complete.columns)\n",
    "df_consensus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to get different number of clusters\n",
    "\n",
    "# Minimum acceptable value for robust clusters\n",
    "thresh = 0.7\n",
    "\n",
    "# change this to get a different linkage (by method)\n",
    "df_consensus_dist = 1 - df_consensus_matrix\n",
    "link = hc.linkage(scipy.spatial.distance.squareform(df_consensus_dist), method='ward')\n",
    "\n",
    "# retrieve clusters using fcluster\n",
    "dist = scipy.spatial.distance.squareform(df_consensus_dist)\n",
    "\n",
    "consensus_clst = pd.DataFrame(index=df_acc_complete.columns)\n",
    "consensus_clst['cluster'] = hc.fcluster(link, thresh * dist.max(), 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_clst.cluster.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot showing sizes of each consensus strain cluster (at thresh = 0.5)\n",
    "px.bar(\n",
    "    x=consensus_clst.cluster.value_counts().sort_index().index,\n",
    "    y=consensus_clst.cluster.value_counts().sort_index().values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color each NMF cluster (default matplotlib colors)\n",
    "\n",
    "#cm = matplotlib.colormaps.get_cmap('tab20')\n",
    "cmb = matplotlib.colormaps.get_cmap('tab20b')\n",
    "cmc = matplotlib.colormaps.get_cmap('tab20c')\n",
    "cm_colors = cmb.colors + cmc.colors\n",
    "\n",
    "consensus_clr = dict(zip(sorted(consensus_clst.cluster.unique()), cm_colors))\n",
    "consensus_clst['color'] = consensus_clst.cluster.map(consensus_clr)\n",
    "\n",
    "print('Number of colors: ', len(consensus_clr))\n",
    "print('Number of clusters', len(consensus_clst.cluster.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 9\n",
    "\n",
    "#legend_TN = [patches.Patch(color=c, label=l) for l,c in mash_color_dict_31.items()] # Mash cluster for legend\n",
    "\n",
    "sns.set(rc={'figure.facecolor':'white'})\n",
    "g = sns.clustermap(\n",
    "    df_consensus_matrix,\n",
    "    figsize=(size,size),\n",
    "    row_linkage=link,\n",
    "    #row_colors=phylogroup_clst.color, # Phylogroup colors on left\n",
    "    col_linkage=link,\n",
    "    #col_colors=clst.color, # Mash cluster on top\n",
    "    yticklabels=False,\n",
    "    xticklabels=False,\n",
    "    cmap='hot_r'\n",
    ")\n",
    "\n",
    "#l2=g.ax_heatmap.legend(loc='upper left', bbox_to_anchor=(1.01,0.75), handles=legend_TN, frameon=True)\n",
    "#l2.set_title(title='Mash cluster',prop={'size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper diagonal elements of consensus\n",
    "avec = np.array([consensus_matrix[i, j] for i in range(consensus_matrix.shape[0] - 1)\n",
    "                 for j in range(i + 1, consensus_matrix.shape[1])])\n",
    "\n",
    "# consensus entries are similarities, conversion to distances\n",
    "Y = 1 - avec\n",
    "Z = hc.linkage(Y, method='ward')\n",
    "\n",
    "# cophenetic correlation coefficient of a hierarchical clustering\n",
    "# defined by the linkage matrix Z and matrix Y from which Z was\n",
    "# generated\n",
    "coph_cor, _ = cophenet(Z, Y)\n",
    "\n",
    "coph_cor # Cophenetic correlation of consensus matrix (ideally 0.7 or higher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion = np.sum(4 * np.multiply(consensus_matrix - 0.5, consensus_matrix - 0.5)) / consensus_matrix.size\n",
    "\n",
    "dispersion # Dispersion coefficient of consensus matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After removing small-rank models ((50, 75) and (75,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_dict_filt = dict.fromkeys(P_submatrices.keys())\n",
    "make_dict_in_dict(conn_dict_filt)\n",
    "\n",
    "for min_key in P_submatrices.keys():\n",
    "    for max_key in P_submatrices.keys():\n",
    "        if min_key == 0 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key == 50 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key == 75 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key < max_key:\n",
    "            conn_dict_filt[min_key][max_key] = connectivity(\n",
    "                P_submatrices[min_key][max_key].values,\n",
    "                A_binarized_submatrices[min_key][max_key][best_ranks_dict[(min_key, max_key)]].values\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consensus matrix for these runs (H matrix, default)\n",
    "consensus_matrix_filt = np.zeros(shape=conn_dict_filt[0][25].shape)\n",
    "\n",
    "num_conn_mat = 0\n",
    "for min_key in P_submatrices.keys():\n",
    "    for max_key in P_submatrices.keys():\n",
    "        if min_key == 0 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key == 50 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key == 75 and max_key == 100:\n",
    "            continue\n",
    "        elif min_key < max_key:\n",
    "            num_conn_mat += 1\n",
    "            consensus_matrix_filt += conn_dict_filt[min_key][max_key]\n",
    "\n",
    "consensus_matrix_filt /= num_conn_mat\n",
    "\n",
    "df_consensus_matrix_filt = pd.DataFrame(consensus_matrix_filt, index=df_acc_complete.columns, columns=df_acc_complete.columns)\n",
    "df_consensus_matrix_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to get different number of clusters\n",
    "\n",
    "# Minimum acceptable value for robust clusters = 50%\n",
    "thresh = 0.7\n",
    "\n",
    "# change this to get a different linkage (by method)\n",
    "df_consensus_filt_dist = 1 - df_consensus_matrix_filt\n",
    "link = hc.linkage(scipy.spatial.distance.squareform(df_consensus_filt_dist), method='ward')\n",
    "\n",
    "# retrieve clusters using fcluster\n",
    "dist = scipy.spatial.distance.squareform(df_consensus_filt_dist)\n",
    "\n",
    "consensus_clst_filt = pd.DataFrame(index=df_acc_complete.columns)\n",
    "consensus_clst_filt['cluster'] = hc.fcluster(link, thresh * dist.max(), 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_clst_filt.cluster.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot showing sizes of each consensus strain cluster (at thresh = 0.5)\n",
    "px.bar(\n",
    "    x=consensus_clst_filt.cluster.value_counts().sort_index().index,\n",
    "    y=consensus_clst_filt.cluster.value_counts().sort_index().values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color each NMF cluster (default matplotlib colors)\n",
    "\n",
    "#cm = matplotlib.colormaps.get_cmap('tab20')\n",
    "cmb = matplotlib.colormaps.get_cmap('tab20b')\n",
    "cmc = matplotlib.colormaps.get_cmap('tab20c')\n",
    "cm_colors = cmb.colors + cmc.colors\n",
    "\n",
    "consensus_clr_filt = dict(zip(sorted(consensus_clst_filt.cluster.unique()), cm_colors))\n",
    "consensus_clst_filt['color'] = consensus_clst_filt.cluster.map(consensus_clr_filt)\n",
    "\n",
    "print('Number of colors: ', len(consensus_clr_filt))\n",
    "print('Number of clusters', len(consensus_clst_filt.cluster.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 9\n",
    "\n",
    "#legend_TN = [patches.Patch(color=c, label=l) for l,c in mash_color_dict_31.items()] # Mash cluster for legend\n",
    "\n",
    "sns.set(rc={'figure.facecolor':'white'})\n",
    "g = sns.clustermap(\n",
    "    df_consensus_matrix_filt,\n",
    "    figsize=(size,size),\n",
    "    row_linkage=link,\n",
    "    #row_colors=phylogroup_clst.color, # Phylogroup colors on left\n",
    "    col_linkage=link,\n",
    "    #col_colors=clst.color, # Mash cluster on top\n",
    "    yticklabels=False,\n",
    "    xticklabels=False,\n",
    "    cmap='hot_r'\n",
    ")\n",
    "\n",
    "#l2=g.ax_heatmap.legend(loc='upper left', bbox_to_anchor=(1.01,0.75), handles=legend_TN, frameon=True)\n",
    "#l2.set_title(title='Mash cluster',prop={'size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper diagonal elements of consensus\n",
    "avec = np.array([consensus_matrix_filt[i, j] for i in range(consensus_matrix_filt.shape[0] - 1)\n",
    "                 for j in range(i + 1, consensus_matrix_filt.shape[1])])\n",
    "\n",
    "# consensus entries are similarities, conversion to distances\n",
    "Y = 1 - avec\n",
    "Z = hc.linkage(Y, method='ward')\n",
    "\n",
    "# cophenetic correlation coefficient of a hierarchical clustering\n",
    "# defined by the linkage matrix Z and matrix Y from which Z was\n",
    "# generated\n",
    "coph_cor, _ = cophenet(Z, Y)\n",
    "\n",
    "coph_cor # Cophenetic correlation of consensus matrix (ideally 0.7 or higher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion = np.sum(4 * np.multiply(consensus_matrix_filt - 0.5, consensus_matrix_filt - 0.5)) / consensus_matrix_filt.size\n",
    "\n",
    "dispersion # Dispersion coefficient of consensus matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(consensus_clst.cluster.value_counts() >= 10).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding robust sets across ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_L_norm_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best run for main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_norm = best_L_norm_dict['full']\n",
    "A_norm = best_A_norm_dict['full']\n",
    "\n",
    "L_bin = best_L_binarized_dict['full']\n",
    "A_bin = best_A_binarized_dict['full']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save NMF outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = os.path.join(WORKDIR, 'processed/nmf-outputs/') \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "L_norm.to_csv(os.path.join(WORKDIR, 'processed/nmf-outputs/L.csv'))\n",
    "A_norm.to_csv(os.path.join(WORKDIR, 'processed/nmf-outputs/A.csv'))\n",
    "\n",
    "L_bin.to_csv(os.path.join(WORKDIR, 'processed/nmf-outputs/L_binarized.csv'))\n",
    "A_bin.to_csv(os.path.join(WORKDIR, 'processed/nmf-outputs/A_binarized.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyphylon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
